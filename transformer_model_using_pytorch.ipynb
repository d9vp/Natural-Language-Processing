{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMIkxPepwI6bepE7+1v0ErS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"SnQxYNcwn1Mu","executionInfo":{"status":"ok","timestamp":1699878223764,"user_tz":-330,"elapsed":5134,"user":{"displayName":"Dev Patel","userId":"06579500276645906848"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Transformer Block Module\n"],"metadata":{"id":"0sWPWUUnn4Aq"}},{"cell_type":"code","source":["class TransformerLayer(nn.Module):\n","    def __init__(self, embed_dim, expansion_factor=4, num_heads=8):\n","        super(TransformerLayer, self).__init__()\n","\n","        self.self_attention = nn.MultiheadAttention(embed_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","        self.feed_forward = nn.Sequential(\n","                          nn.Linear(embed_dim, expansion_factor * embed_dim),\n","                          nn.ReLU(),\n","                          nn.Linear(expansion_factor * embed_dim, embed_dim)\n","        )\n","        self.dropout1 = nn.Dropout(0.2)\n","        self.dropout2 = nn.Dropout(0.2)\n","\n","    def forward(self, key, query, value):\n","        attention_out, _ = self.self_attention(key, query, value) # Multi-head self-attention\n","        attention_residual_out = attention_out + value\n","        norm1_out = self.dropout1(self.norm1(attention_residual_out))\n","\n","        # Feed-forward network\n","        feed_fwd_out = self.feed_forward(norm1_out)\n","        feed_fwd_residual_out = feed_fwd_out + norm1_out\n","        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out))\n","\n","        return norm2_out"],"metadata":{"id":"K780VErln6J9","executionInfo":{"status":"ok","timestamp":1699878223764,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dev Patel","userId":"06579500276645906848"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Transformer Encoder Module"],"metadata":{"id":"93V440ILn74p"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, num_heads=8):\n","        super(TransformerEncoder, self).__init__()\n","        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n","        self.positional_encoder = nn.Embedding(seq_len, embed_dim)  # Simplified positional encoding\n","        self.layers = nn.ModuleList([TransformerLayer(embed_dim, expansion_factor, num_heads) for _ in range(num_layers)]) # Stack multiple transformer layers\n","\n","    def forward(self, x):\n","        embedded_text = self.embedding_layer(x) # Word embedding\n","        out = embedded_text + self.positional_encoder.weight.unsqueeze(0) # Simplified positional encoding\n","        for layer in self.layers: # Transformer layers\n","            out = layer(out, out, out)\n","        return out"],"metadata":{"id":"fLEpP8-tn9lG","executionInfo":{"status":"ok","timestamp":1699878223764,"user_tz":-330,"elapsed":5,"user":{"displayName":"Dev Patel","userId":"06579500276645906848"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Transformer Decoder Module\n"],"metadata":{"id":"6tLeiYJxn--C"}},{"cell_type":"code","source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, num_heads=8):\n","        super(TransformerDecoder, self).__init__()\n","\n","        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n","        self.positional_encoder = nn.Embedding(seq_len, embed_dim)  # Simplified positional encoding\n","        self.layers = nn.ModuleList([TransformerLayer(embed_dim, expansion_factor, num_heads) for _ in range(num_layers)])\n","        self.embed_dim = embed_dim\n","\n","    def forward(self, x, encoder_output):\n","        embedded_text = self.embedding_layer(x) # Word embedding for target sequences\n","        out = embedded_text + self.positional_encoder.weight.unsqueeze(0) # Simplified positional encoding\n","\n","        for layer in self.layers: # Transformer layers in the decoder\n","            out = layer(out, out, out)\n","\n","        linear_layer = nn.Linear(self.embed_dim, tgt_vocab_size)  # Use self.embed_dim\n","        final_predictions = linear_layer(out)\n","\n","        return final_predictions"],"metadata":{"id":"InXkLMhEoAsk","executionInfo":{"status":"ok","timestamp":1699878223764,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dev Patel","userId":"06579500276645906848"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Modified Transformer Encoder-Decoder for Machine Translation\n"],"metadata":{"id":"J6sXE7eFoCFf"}},{"cell_type":"code","source":["class TransformerEncoderDecoder(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, num_layers=2, expansion_factor=4, num_heads=8):\n","        super(TransformerEncoderDecoder, self).__init__()\n","\n","        # Encoder for source sequences\n","        self.encoder = TransformerEncoder(\n","            seq_len=max_seq_length,\n","            vocab_size=src_vocab_size,\n","            embed_dim=embed_dim,\n","            num_layers=num_layers,\n","            expansion_factor=expansion_factor,\n","            num_heads=num_heads\n","        )\n","\n","        # Decoder for target sequences\n","        self.decoder = TransformerDecoder(\n","            seq_len=max_seq_length,\n","            vocab_size=tgt_vocab_size,\n","            embed_dim=embed_dim,\n","            num_layers=num_layers,\n","            expansion_factor=expansion_factor,\n","            num_heads=num_heads\n","        )\n","\n","    def forward(self, source_sequences, target_sequences):\n","        encoder_output = self.encoder(source_sequences) # Forward pass through the encoder\n","        final_predictions = self.decoder(target_sequences, encoder_output) # Forward pass through the decoder\n","        return final_predictions"],"metadata":{"id":"QXf_xx1moE5E","executionInfo":{"status":"ok","timestamp":1699878223764,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dev Patel","userId":"06579500276645906848"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Example Usage: Machine Translation with Transformer Encoder-Decoder"],"metadata":{"id":"Ns2HfeuzoUsm"}},{"cell_type":"code","source":["src_vocab_size = 5000\n","tgt_vocab_size = 5000\n","max_seq_length = 30\n","batch_size = 4\n","\n","# Create random tensors representing a batch of source and target sequences\n","source_sequences = torch.randint(0, src_vocab_size, (batch_size, max_seq_length))\n","target_sequences = torch.randint(0, tgt_vocab_size, (batch_size, max_seq_length))\n","\n","# Create a Transformer Encoder-Decoder model for machine translation\n","translation_model = TransformerEncoderDecoder(\n","    src_vocab_size=src_vocab_size,\n","    tgt_vocab_size=tgt_vocab_size,\n","    embed_dim=512,\n","    num_layers=4,\n","    expansion_factor=4,\n","    num_heads=8\n",")\n","\n","# Forward pass through the model to get translation predictions\n","translation_predictions = translation_model(source_sequences, target_sequences)"],"metadata":{"id":"cDoedswNoHG1","executionInfo":{"status":"ok","timestamp":1699878224542,"user_tz":-330,"elapsed":781,"user":{"displayName":"Dev Patel","userId":"06579500276645906848"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["translation_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lU3fKbk4oZ-H","executionInfo":{"status":"ok","timestamp":1699878224542,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dev Patel","userId":"06579500276645906848"}},"outputId":"cf1e405b-d944-41ce-87b0-dadea99539b9"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TransformerEncoderDecoder(\n","  (encoder): TransformerEncoder(\n","    (embedding_layer): Embedding(5000, 512)\n","    (positional_encoder): Embedding(30, 512)\n","    (layers): ModuleList(\n","      (0-3): 4 x TransformerLayer(\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (feed_forward): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (dropout1): Dropout(p=0.2, inplace=False)\n","        (dropout2): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoder(\n","    (embedding_layer): Embedding(5000, 512)\n","    (positional_encoder): Embedding(30, 512)\n","    (layers): ModuleList(\n","      (0-3): 4 x TransformerLayer(\n","        (self_attention): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (feed_forward): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","        )\n","        (dropout1): Dropout(p=0.2, inplace=False)\n","        (dropout2): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Print the shape of the translation predictions\n","print(\"Translation Predictions Shape:\", translation_predictions.shape)\n","print(\"\\n\\n\",translation_predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mh9kN5Zrocwp","executionInfo":{"status":"ok","timestamp":1699878224542,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dev Patel","userId":"06579500276645906848"}},"outputId":"5cb2b6f7-2ef6-461f-a6cc-69e40c9c6489"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Translation Predictions Shape: torch.Size([4, 30, 5000])\n","\n","\n"," tensor([[[-0.2388, -0.3183, -0.1799,  ...,  0.3858,  0.0165, -0.6137],\n","         [ 0.2497, -0.3958,  0.2757,  ...,  1.0989, -0.6485,  0.2483],\n","         [ 0.0302, -0.3694,  0.1863,  ..., -0.5506,  1.0771, -0.7228],\n","         ...,\n","         [-0.4957, -0.0409, -0.2502,  ..., -0.2347, -0.4575, -0.0296],\n","         [ 1.2763,  0.5245, -0.6327,  ..., -0.0232,  1.4271,  0.1770],\n","         [ 0.6265, -0.2581,  0.1957,  ...,  0.0897, -0.0408,  0.0698]],\n","\n","        [[-0.1906,  0.5440, -0.1165,  ...,  0.1000, -0.2049, -0.3806],\n","         [ 1.4077,  0.4157,  0.2377,  ..., -0.3097,  0.9587, -0.1685],\n","         [-0.1463, -0.0877,  0.2488,  ...,  0.7598,  1.1981,  1.4609],\n","         ...,\n","         [ 0.7917,  0.6685,  1.4523,  ...,  0.8139, -0.2970, -0.5302],\n","         [ 0.2495,  0.1917, -1.2684,  ...,  0.1425, -0.3022,  1.0394],\n","         [-0.2949, -0.5780, -0.0741,  ...,  0.3656, -0.6301,  0.3284]],\n","\n","        [[-0.2424,  0.5086,  0.0317,  ..., -0.2032,  0.2504, -0.4485],\n","         [-1.0035,  0.9875, -0.2949,  ..., -0.2592, -0.6399,  0.1688],\n","         [-0.6182, -0.8453, -0.3982,  ...,  0.5681,  0.7487,  0.2581],\n","         ...,\n","         [ 0.5337,  0.1695,  0.0842,  ...,  0.4848,  0.1892,  0.8358],\n","         [-0.0943,  1.0096,  0.3297,  ...,  0.0359,  0.4235, -1.2008],\n","         [-0.1646, -0.9697,  0.5656,  ..., -0.2747, -0.4249, -0.1973]],\n","\n","        [[ 0.1706, -0.7438, -1.2167,  ..., -0.2837,  0.2766, -0.9944],\n","         [ 0.9627,  0.0674,  0.1010,  ..., -0.0387,  0.1642,  0.0710],\n","         [ 0.1690, -0.6195,  0.7905,  ...,  0.8121,  0.5029, -0.3153],\n","         ...,\n","         [ 0.3544,  0.7394,  0.3545,  ...,  0.4254, -0.5596, -1.0146],\n","         [-0.5874,  0.0039, -1.6873,  ...,  0.8789,  0.2927,  0.1596],\n","         [ 0.5003, -1.0804,  0.2018,  ...,  0.1452, -0.3019,  0.0101]]],\n","       grad_fn=<ViewBackward0>)\n"]}]}]}